library("neuralnet")
> 
> #Going to create a neural network to perform sqare rooting
> #Type ?neuralnet for more information on the neuralnet library
> 
> #Generate 50 random numbers uniformly distributed between 0 and 100
> #And store them as a dataframe
> traininginput <-  as.data.frame(runif(50, min=1, max=16))
> trainingoutput <- exp(sqrt(traininginput))
> 
> #Column bind the data into one variable
> trainingdata <- cbind(traininginput,trainingoutput)
> colnames(trainingdata) <- c("Input","Output")
> 
> #Train the neural network
> #Going to have 10 hidden layers
> #Threshold is a numeric value specifying the threshold for the partial
> #derivatives of the error function as stopping criteria.
> net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=c(8,5,3), threshold=0.01)
Warning message:
algorithm did not converge in 1 of 1 repetition(s) within the stepmax 
> print(net.sqrt)
$call
neuralnet(formula = Output ~ Input, data = trainingdata, hidden = c(8, 
    5, 3), threshold = 0.01)

$response
         Output
1  19.398839025
2  10.225415949
3  19.588071380
4  31.641479667
5  19.327393286
6  28.285705943
7  13.764402243
8  53.924738216
9  24.940527267
10 25.553044973
11 23.009781425
12 13.050719746
13 39.496171681
14  2.756377485
15 18.404027938
16 37.151503156
17 17.480517148
18 35.146907215
19  9.529519415
20 31.703155831
21 16.506014507
22 49.158650708
23 28.482111917
24 41.393821363
25 32.796602231
26 24.419125075
27 30.959995696
28 17.849604956
29 16.941445294
30 15.911557269
31  4.412793874
32 12.279979321
33  7.589854583
34  2.886258261
35 45.439386940
36  5.509352719
37 28.678429109
38 11.186579638
39 13.909442491
40 25.502902568
41 38.068311688
42  3.573146330
43  4.754601760
44 11.222866271
45 30.174240187
46 49.932918723
47 23.458718996
48  5.951902105
49 17.972432216
50 39.146391073

$covariate
              [,1]
 [1,]  8.792489441
 [2,]  5.405050186
 [3,]  8.850153635
 [4,] 11.933355433
 [5,]  8.770621032
 [6,] 11.171347553
 [7,]  6.875333480
 [8,] 15.900868726
 [9,] 10.345834176
[10,] 10.502502605
[11,]  9.833990514
[12,]  6.598955824
[13,] 13.514473995
[14,]  1.028028316
[15,]  8.483061386
[16,] 13.068255598
[17,]  8.185818563
[18,] 12.670300993
[19,]  5.082293605
[20,] 11.946813148
[21,]  7.860872923
[22,] 15.171436606
[23,] 11.217651320
[24,] 13.861709114
[25,] 12.182368044
[26,] 10.210367963
[27,] 11.783401208
[28,]  8.305816655
[29,]  8.007558659
[30,]  7.656542002
[31,]  2.203764062
[32,]  6.289914719
[33,]  4.107968635
[34,]  1.123517202
[35,] 14.564750829
[36,]  2.911961850
[37,] 11.263710880
[38,]  5.830847629
[39,]  6.930413882
[40,] 10.489775380
[41,] 13.245102957
[42,]  1.621666071
[43,]  2.430833162
[44,]  5.846498298
[45,] 11.607571228
[46,] 15.293421810
[47,]  9.955553580
[48,]  3.181624396
[49,]  8.345391001
[50,] 13.449149699

$model.list
$model.list$response
[1] "Output"

$model.list$variables
[1] "Input"


$err.fct
function (x, y) 
{
    1/2 * (y - x)^2
}
<bytecode: 0x000000000c6d8808>
<environment: 0x000000000cf85b98>
attr(,"type")
[1] "sse"

$act.fct
function (x) 
{
    1/(1 + exp(-x))
}
<bytecode: 0x000000000d62d258>
<environment: 0x000000000cf85b98>
attr(,"type")
[1] "logistic"

$linear.output
[1] TRUE

$data
          Input       Output
1   8.792489441 19.398839025
2   5.405050186 10.225415949
3   8.850153635 19.588071380
4  11.933355433 31.641479667
5   8.770621032 19.327393286
6  11.171347553 28.285705943
7   6.875333480 13.764402243
8  15.900868726 53.924738216
9  10.345834176 24.940527267
10 10.502502605 25.553044973
11  9.833990514 23.009781425
12  6.598955824 13.050719746
13 13.514473995 39.496171681
14  1.028028316  2.756377485
15  8.483061386 18.404027938
16 13.068255598 37.151503156
17  8.185818563 17.480517148
18 12.670300993 35.146907215
19  5.082293605  9.529519415
20 11.946813148 31.703155831
21  7.860872923 16.506014507
22 15.171436606 49.158650708
23 11.217651320 28.482111917
24 13.861709114 41.393821363
25 12.182368044 32.796602231
26 10.210367963 24.419125075
27 11.783401208 30.959995696
28  8.305816655 17.849604956
29  8.007558659 16.941445294
30  7.656542002 15.911557269
31  2.203764062  4.412793874
32  6.289914719 12.279979321
33  4.107968635  7.589854583
34  1.123517202  2.886258261
35 14.564750829 45.439386940
36  2.911961850  5.509352719
37 11.263710880 28.678429109
38  5.830847629 11.186579638
39  6.930413882 13.909442491
40 10.489775380 25.502902568
41 13.245102957 38.068311688
42  1.621666071  3.573146330
43  2.430833162  4.754601760
44  5.846498298 11.222866271
45 11.607571228 30.174240187
46 15.293421810 49.932918723
47  9.955553580 23.458718996
48  3.181624396  5.951902105
49  8.345391001 17.972432216
50 13.449149699 39.146391073

attr(,"class")
[1] "nn"
> 
> #Plot the neural network
> plot(net.sqrt)
Error in plot.nn(net.sqrt) : weights were not calculated
> 
> #Test the neural network on some training data
> testdata <- as.data.frame((1:10)^2) #Generate some squared numbers
> net.results <- compute(net.sqrt, testdata) #Run them through the neural network
Error in nrow[w] * ncol[w] : non-numeric argument to binary operator
In addition: Warning message:
In is.na(weights) : is.na() applied to non-(list or vector) of type 'NULL'
> 
> #Lets see what properties net.sqrt has
> ls(net.results)
[1] "net.result" "neurons"   
> 
> #Lets see the results
> print(net.results$net.result)
              [,1]
 [1,]  2.980122198
 [2,]  7.393467981
 [3,] 20.096673718
 [4,] 54.434714628
 [5,] 63.343246909
 [6,] 63.392688977
 [7,] 63.389077045
 [8,] 63.387980065
 [9,] 63.387818922
[10,] 63.387802238
> 
> #Lets display a better version of the results
> cleanoutput <- cbind(testdata,exp(sqrt(testdata)),
+                      as.data.frame(net.results$net.result))
> colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
> print(cleanoutput)
   Input Expected Output Neural Net Output
1      1     2.718281828       2.980122198
2      4     7.389056099       7.393467981
3      9    20.085536923      20.096673718
4     16    54.598150033      54.434714628
5     25   148.413159103      63.343246909
6     36   403.428793493      63.392688977
7     49  1096.633158428      63.389077045
8     64  2980.957987042      63.387980065
9     81  8103.083927575      63.387818922
10   100 22026.465794807      63.387802238